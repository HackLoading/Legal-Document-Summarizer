{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f6d9c3",
   "metadata": {},
   "source": [
    "# LED Legal Summarization with Full Argument Labelling Integration\n",
    "\n",
    "This notebook implements a complete pipeline for training a LED model on legal summarization data, fully integrating argument labelling to filter and augment inputs for better argumentative coherence in summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00f795e",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab581be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Atharva Badgujar\\Downloads\\arglegalsumm-master\\arglegalsumm-master\\webapp\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Atharva\n",
      "[nltk_data]     Badgujar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    LEDTokenizer, LEDForConditionalGeneration,\n",
    "    Seq2SeqTrainer, Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e62ba6",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f9bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = \".\"\n",
    "SRC_DIR = \"src\"\n",
    "MODEL_DIR = \"models\"\n",
    "CONFIG_FILE = os.path.join(MODEL_DIR, \"config.json\")\n",
    "\n",
    "# Load config\n",
    "def load_cfg():\n",
    "    if os.path.exists(CONFIG_FILE):\n",
    "        with open(CONFIG_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {\n",
    "        \"model\": {\"checkpoint\": \"allenai/led-base-16384\"},\n",
    "        \"data_schema\": {\"input_col\": \"Text\", \"summary_col\": \"Summary\"},\n",
    "        \"data\": {\"max_input_length\": 16384, \"max_output_length\": 512}\n",
    "    }\n",
    "\n",
    "cfg = load_cfg()\n",
    "\n",
    "# Load datasets\n",
    "data_files = {\"train\": \"train (1).csv\", \"test\": \"test.csv\"}\n",
    "raw_datasets = load_dataset(\"csv\", data_files=data_files)\n",
    "print(\"Dataset loaded:\", raw_datasets)\n",
    "print(\"Train sample:\", raw_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836dd9cb",
   "metadata": {},
   "source": [
    "## 3. Preprocess the Data (with Argument Labelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a87b6d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SRC_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load argument predictions\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m preds_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mSRC_DIR\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margument_classification\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124martifacts\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlegal_bert_predicts.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(preds_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      4\u001b[0m     argument_labels \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines()]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SRC_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "# Load argument predictions\n",
    "preds_path = os.path.join(SRC_DIR, 'argument_classification', 'artifacts', 'legal_bert_predicts.txt')\n",
    "with open(preds_path, 'r') as f:\n",
    "    argument_labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f\"Loaded {len(argument_labels)} argument labels.\")\n",
    "\n",
    "# Function to preprocess with argument labelling\n",
    "def preprocess_with_arguments(batch, labels, input_col, summary_col, start_idx=0):\n",
    "    processed_batch = []\n",
    "    label_idx = start_idx\n",
    "    for text, summary in zip(batch[input_col], batch[summary_col]):\n",
    "        sentences = sent_tokenize(text)\n",
    "        filtered_sentences = []\n",
    "        for sent in sentences:\n",
    "            if label_idx < len(labels) and labels[label_idx] in ['Issue', 'Reason', 'Conclusion']:\n",
    "                filtered_sentences.append(sent)\n",
    "            label_idx += 1\n",
    "        # If no argumentative sentences, keep original\n",
    "        if not filtered_sentences:\n",
    "            filtered_sentences = sentences[:5]  # Fallback to first 5\n",
    "        augmented_text = \" \".join(filtered_sentences)\n",
    "        processed_batch.append({input_col: augmented_text, summary_col: summary})\n",
    "    return processed_batch, label_idx\n",
    "\n",
    "# Apply to datasets\n",
    "train_data, train_end_idx = preprocess_with_arguments(raw_datasets['train'], argument_labels, cfg['data_schema']['input_col'], cfg['data_schema']['summary_col'])\n",
    "test_data, _ = preprocess_with_arguments(raw_datasets['test'], argument_labels, cfg['data_schema']['input_col'], cfg['data_schema']['summary_col'], start_idx=train_end_idx)\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "print(\"Preprocessed train sample:\", train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eb2c15",
   "metadata": {},
   "source": [
    "## 4. Tokenize and Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57762163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = LEDTokenizer.from_pretrained(cfg['model']['checkpoint'])\n",
    "model = LEDForConditionalGeneration.from_pretrained(cfg['model']['checkpoint'])\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(batch):\n",
    "    inputs = tokenizer(batch[cfg['data_schema']['input_col']], truncation=True, padding='max_length', max_length=cfg['data']['max_input_length'])\n",
    "    targets = tokenizer(batch[cfg['data_schema']['summary_col']], truncation=True, padding='max_length', max_length=cfg['data']['max_output_length'])\n",
    "    inputs['labels'] = targets['input_ids']\n",
    "    # Global attention for LED\n",
    "    inputs['global_attention_mask'] = [[1 if i == 0 else 0 for i in range(len(ids))] for ids in inputs['input_ids']]\n",
    "    return inputs\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=[cfg['data_schema']['input_col'], cfg['data_schema']['summary_col']])\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True, remove_columns=[cfg['data_schema']['input_col'], cfg['data_schema']['summary_col']])\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c295db",
   "metadata": {},
   "source": [
    "## 5. Build and Train the Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aef2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "# Training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./led_argument_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f2775",
   "metadata": {},
   "source": [
    "## 6. Evaluate the Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b8736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7929a5",
   "metadata": {},
   "source": [
    "## 7. Save and Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede2496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trainer.save_model(\"./led_argument_model_final\")\n",
    "tokenizer.save_pretrained(\"./led_argument_model_final\")\n",
    "\n",
    "# Load for inference\n",
    "loaded_model = LEDForConditionalGeneration.from_pretrained(\"./led_argument_model_final\")\n",
    "loaded_tokenizer = LEDTokenizer.from_pretrained(\"./led_argument_model_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceb0caa",
   "metadata": {},
   "source": [
    "## 8. Check if Retraining is Needed and Retrain if Necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c03bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check performance threshold\n",
    "rouge_l_score = eval_results.get('eval_rougeL', 0)\n",
    "threshold = 50.0  # Example threshold\n",
    "\n",
    "if rouge_l_score < threshold:\n",
    "    print(f\"ROUGE-L score {rouge_l_score} below threshold {threshold}. Retraining...\")\n",
    "    # Retrain with more epochs or different params\n",
    "    training_args.num_train_epochs = 5\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=loaded_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_test,\n",
    "        tokenizer=loaded_tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model(\"./led_argument_model_retrained\")\n",
    "else:\n",
    "    print(f\"Model performance satisfactory: ROUGE-L {rouge_l_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
