{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92907c8c",
   "metadata": {},
   "source": [
    "# ArgLegalSumm: End-to-end Legal Text Summarization (Colab-ready)\n",
    "\n",
    "This single notebook trains a Hugging Face encoderâ€“decoder model on your legal dataset (train/test CSVs), resumes safely from checkpoints after restarts, evaluates ROUGE, and runs inference. Steps are idempotent and re-load settings from disk so each cell can run independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b4198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Check GPU and Install Dependencies (safe to rerun)\n",
    "import sys, subprocess, json, os\n",
    "\n",
    "REQS = [\n",
    "    'transformers>=4.41.0',\n",
    "    'datasets>=2.18.0',\n",
    "    'accelerate>=0.32.0',\n",
    "    'evaluate>=0.4.2',\n",
    "    'rouge-score>=0.1.2',\n",
    "    'sentencepiece>=0.1.99',\n",
    "    'PyYAML>=6.0.1',\n",
    "    'pandas>=2.0.0',\n",
    "    'tqdm>=4.66.0',\n",
    "    'nltk>=3.8.1',\n",
    "    'scikit-learn>=1.2.0'\n",
    "]\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    cmd = [sys.executable, '-m', 'pip', 'install', '-q'] + pkgs\n",
    "    try:\n",
    "        subprocess.check_call(cmd)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print('pip install error:', e)\n",
    "\n",
    "pip_install(REQS)\n",
    "\n",
    "# Show versions\n",
    "import transformers, datasets, evaluate, pandas as pd, torch, nltk\n",
    "print('Python:', sys.version.split()[0])\n",
    "print('Torch:', torch.__version__)\n",
    "print('Transformers:', transformers.__version__)\n",
    "print('Datasets:', datasets.__version__)\n",
    "print('Evaluate:', evaluate.__version__)\n",
    "\n",
    "# GPU info\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA available. Device count:', torch.cuda.device_count())\n",
    "    print('Current device:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('CUDA not available; training will run on CPU (slow).')\n",
    "\n",
    "# NLTK punkt for optional scoring\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    import nltk\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22a73a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Connect to Google Drive and Define Paths (persistent artifacts)\n",
    "import os, json\n",
    "\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    ROOT_DIR = '/content/drive/MyDrive/arglegalsumm'\n",
    "else:\n",
    "    # Fallback local path if running outside Colab\n",
    "    ROOT_DIR = os.path.abspath('./arglegalsumm_artifacts')\n",
    "\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data')\n",
    "CKPT_DIR = os.path.join(ROOT_DIR, 'checkpoints')\n",
    "BEST_DIR = os.path.join(ROOT_DIR, 'best')\n",
    "LOG_DIR = os.path.join(ROOT_DIR, 'logs')\n",
    "TOKENIZER_DIR = os.path.join(ROOT_DIR, 'tokenizer')\n",
    "CACHE_DIR = os.path.join(ROOT_DIR, 'cache')\n",
    "EXPORT_DIR = os.path.join(ROOT_DIR, 'exports')\n",
    "SRC_DIR = os.path.join(ROOT_DIR, 'src')  # optional copy of the project source\n",
    "\n",
    "for d in [ROOT_DIR, DATA_DIR, CKPT_DIR, BEST_DIR, LOG_DIR, TOKENIZER_DIR, CACHE_DIR, EXPORT_DIR, SRC_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print('ROOT_DIR:', ROOT_DIR)\n",
    "print('DATA_DIR:', DATA_DIR)\n",
    "print('CKPT_DIR:', CKPT_DIR)\n",
    "print('BEST_DIR:', BEST_DIR)\n",
    "print('SRC_DIR:', SRC_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9456a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Sync Project Files and Python Path (optional)\n",
    "import os, sys, shutil, glob\n",
    "\n",
    "# Option A (recommended): Keep this notebook self-contained; no import from src required.\n",
    "# Option B: If you have the repo 'src' folder (e.g., uploaded/unzipped to content), copy it into ROOT_DIR and add to sys.path.\n",
    "if os.path.isdir('/content/src'):\n",
    "    # user uploaded repo to /content\n",
    "    if not os.path.isdir(os.path.join(SRC_DIR, 'summarization')):\n",
    "        shutil.copytree('/content/src', SRC_DIR, dirs_exist_ok=True)\n",
    "\n",
    "if os.path.isdir(SRC_DIR):\n",
    "    if SRC_DIR not in sys.path:\n",
    "        sys.path.insert(0, SRC_DIR)\n",
    "\n",
    "print('sys.path contains SRC_DIR:', SRC_DIR in sys.path)\n",
    "print('Found modules under SRC_DIR:', os.listdir(SRC_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0323ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Load Training Config (YAML from repo if available) and persist JSON config\n",
    "import os, json, yaml\n",
    "\n",
    "# Defaults mirror src/summarization/config/train_config.yml\n",
    "DEFAULTS = {\n",
    "    'model': {\n",
    "        'checkpoint': 'allenai/led-base-16384',\n",
    "        'batchsize': 1\n",
    "    },\n",
    "    'data': {\n",
    "        'max_input_length': 1024,\n",
    "        'max_output_length': 512\n",
    "    },\n",
    "    'training': {\n",
    "        'eval_steps': 250,\n",
    "        'save_steps': 250,\n",
    "        'epochs': 3,  # lower by default for Colab; override if YAML present\n",
    "        'gradient_accumulation_steps': 4,\n",
    "        'learning_rate': 5e-5,\n",
    "        'logging_steps': 50\n",
    "    },\n",
    "    'io': {\n",
    "        'output_dir': CKPT_DIR,\n",
    "        'best_dir': BEST_DIR,\n",
    "        'tokenizer_dir': TOKENIZER_DIR,\n",
    "        'export_dir': EXPORT_DIR,\n",
    "        'cache_dir': CACHE_DIR\n",
    "    },\n",
    "    'data_schema': {\n",
    "        'input_col_candidates': ['article', 'text', 'document', 'content'],\n",
    "        'summary_col_candidates': ['summary', 'target', 'abstract']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Try to load YAML from the repo structure if present under SRC_DIR\n",
    "YAML_PATHS = [\n",
    "    os.path.join(SRC_DIR, 'summarization', 'config', 'train_config.yml'),\n",
    "    os.path.join('/content', 'src', 'summarization', 'config', 'train_config.yml'),\n",
    "]\n",
    "loaded_yml = None\n",
    "for yp in YAML_PATHS:\n",
    "    if os.path.isfile(yp):\n",
    "        with open(yp, 'r') as f:\n",
    "            try:\n",
    "                loaded_yml = yaml.safe_load(f)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print('Failed to parse YAML at', yp, e)\n",
    "\n",
    "if loaded_yml:\n",
    "    # merge into defaults\n",
    "    DEFAULTS['model']['checkpoint'] = loaded_yml.get('model', {}).get('checkpoint', DEFAULTS['model']['checkpoint'])\n",
    "    DEFAULTS['model']['batchsize'] = loaded_yml.get('model', {}).get('batchsize', DEFAULTS['model']['batchsize'])\n",
    "    DEFAULTS['data']['max_input_length'] = loaded_yml.get('data', {}).get('max_input_length', DEFAULTS['data']['max_input_length'])\n",
    "    DEFAULTS['data']['max_output_length'] = loaded_yml.get('data', {}).get('max_output_length', DEFAULTS['data']['max_output_length'])\n",
    "    DEFAULTS['training']['eval_steps'] = loaded_yml.get('training', {}).get('eval_steps', DEFAULTS['training']['eval_steps'])\n",
    "    DEFAULTS['training']['save_steps'] = loaded_yml.get('training', {}).get('save_steps', DEFAULTS['training']['save_steps'])\n",
    "    DEFAULTS['training']['epochs'] = loaded_yml.get('training', {}).get('epochs', DEFAULTS['training']['epochs'])\n",
    "\n",
    "# Persist JSON config for restart-safe execution\n",
    "CONFIG_JSON = os.path.join(ROOT_DIR, 'pipeline_config.json')\n",
    "with open(CONFIG_JSON, 'w') as f:\n",
    "    json.dump(DEFAULTS, f, indent=2)\n",
    "\n",
    "print('Resolved config:')\n",
    "print(json.dumps(DEFAULTS, indent=2))\n",
    "print('Saved to', CONFIG_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830de4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Load Train/Test CSVs and Validate Schema (idempotent + case-insensitive + custom paths)\n",
    "import os, json, shutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# reload config\n",
    "with open(os.path.join(ROOT_DIR, 'pipeline_config.json'), 'r') as f:\n",
    "    CFG = json.load(f)\n",
    "\n",
    "# Allow explicit paths via environment variables (recommended in Colab)\n",
    "train_path_env = os.environ.get('TRAIN_CSV_PATH')\n",
    "test_path_env = os.environ.get('TEST_CSV_PATH')\n",
    "\n",
    "# Expected filenames under DATA_DIR if env vars not provided\n",
    "TRAIN_FILE_NAME = 'train (1).csv' if os.path.isfile(os.path.join(DATA_DIR, 'train (1).csv')) else 'train.csv'\n",
    "TEST_FILE_NAME = 'test.csv'\n",
    "\n",
    "if train_path_env and os.path.isfile(train_path_env):\n",
    "    train_path = train_path_env\n",
    "else:\n",
    "    train_path = os.path.join(DATA_DIR, TRAIN_FILE_NAME)\n",
    "\n",
    "if test_path_env and os.path.isfile(test_path_env):\n",
    "    test_path = test_path_env\n",
    "else:\n",
    "    test_path = os.path.join(DATA_DIR, TEST_FILE_NAME)\n",
    "\n",
    "# Provide upload fallback in Colab if files aren't found and no env vars\n",
    "if not (os.path.isfile(train_path) and os.path.isfile(test_path)):\n",
    "    print('CSV files not found. DATA_DIR =', DATA_DIR)\n",
    "    print('You can set TRAIN_CSV_PATH and TEST_CSV_PATH env vars, or upload files now...')\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        uploaded = files.upload()\n",
    "        os.makedirs(DATA_DIR, exist_ok=True)\n",
    "        for name in uploaded.keys():\n",
    "            shutil.move(name, os.path.join(DATA_DIR, name))\n",
    "        train_path = os.path.join(DATA_DIR, TRAIN_FILE_NAME)\n",
    "        test_path = os.path.join(DATA_DIR, TEST_FILE_NAME)\n",
    "        print('Uploaded. DATA_DIR contains:', os.listdir(DATA_DIR))\n",
    "    except Exception as e:\n",
    "        print('Upload failed; please ensure your CSVs exist in DATA_DIR or set env vars.', e)\n",
    "\n",
    "assert os.path.isfile(train_path), f'Missing train CSV: {train_path}'\n",
    "assert os.path.isfile(test_path), f'Missing test CSV: {test_path}'\n",
    "\n",
    "print('Loading CSVs...')\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "print('Train shape:', train_df.shape, '| Test shape:', test_df.shape)\n",
    "print('Train columns:', list(train_df.columns))\n",
    "print('Test columns:', list(test_df.columns))\n",
    "\n",
    "# Case-insensitive normalization of column names\n",
    "train_df.columns = [str(c).strip().lower() for c in train_df.columns]\n",
    "test_df.columns = [str(c).strip().lower() for c in test_df.columns]\n",
    "\n",
    "# Normalize common schema names\n",
    "def resolve_cols(df):\n",
    "    input_col = None\n",
    "    summary_col = None\n",
    "    for c in CFG['data_schema']['input_col_candidates']:\n",
    "        if c.lower() in df.columns:\n",
    "            input_col = c.lower()\n",
    "            break\n",
    "    for c in CFG['data_schema']['summary_col_candidates']:\n",
    "        if c.lower() in df.columns:\n",
    "            summary_col = c.lower()\n",
    "            break\n",
    "    return input_col, summary_col\n",
    "\n",
    "train_in_col, train_sum_col = resolve_cols(train_df)\n",
    "# Prefer summary col on test too (often 'summary')\n",
    "test_in_col, test_sum_col = resolve_cols(test_df)\n",
    "\n",
    "# If test has no summaries, allow test_sum_col to be None\n",
    "assert train_in_col is not None, 'Could not find an input text column (tried: {}) in train CSV'.format(CFG['data_schema']['input_col_candidates'])\n",
    "assert train_sum_col is not None, 'Could not find a summary column (tried: {}) in train CSV'.format(CFG['data_schema']['summary_col_candidates'])\n",
    "\n",
    "# Clean NAs and strip\n",
    "for df, in_c, sum_c in [\n",
    "    (train_df, train_in_col, train_sum_col),\n",
    "    (test_df, test_in_col, test_sum_col),\n",
    "]:\n",
    "    if in_c is not None:\n",
    "        df[in_c] = df[in_c].astype(str).fillna('').str.strip()\n",
    "    if sum_c is not None:\n",
    "        df[sum_c] = df[sum_c].astype(str).fillna('').str.strip()\n",
    "\n",
    "# Persist cleaned copies (idempotent)\n",
    "clean_train_csv = os.path.join(DATA_DIR, 'train_clean.csv')\n",
    "clean_test_csv = os.path.join(DATA_DIR, 'test_clean.csv')\n",
    "train_df.to_csv(clean_train_csv, index=False)\n",
    "test_df.to_csv(clean_test_csv, index=False)\n",
    "\n",
    "# Save resolved schema (lowercase) to config JSON for later cells\n",
    "CFG['data_schema']['input_col'] = train_in_col\n",
    "CFG['data_schema']['summary_col'] = train_sum_col\n",
    "with open(os.path.join(ROOT_DIR, 'pipeline_config.json'), 'w') as f:\n",
    "    json.dump(CFG, f, indent=2)\n",
    "\n",
    "print('Resolved columns -> input:', train_in_col, '| summary:', train_sum_col)\n",
    "print('Saved cleaned datasets to:')\n",
    "print(' -', clean_train_csv)\n",
    "print(' -', clean_test_csv)\n",
    "print('Tip: To use explicit paths next time, set env vars:')\n",
    "print('  TRAIN_CSV_PATH=\"/content/drive/MyDrive/.../train.csv\"  TEST_CSV_PATH=\"/content/drive/MyDrive/.../test.csv\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac9723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Text Cleaning and Normalization (legal-specific; idempotent)\n",
    "import os, re, json\n",
    "import pandas as pd\n",
    "\n",
    "with open(os.path.join(ROOT_DIR, 'pipeline_config.json'), 'r') as f:\n",
    "    CFG = json.load(f)\n",
    "\n",
    "in_col = CFG['data_schema']['input_col']\n",
    "sum_col = CFG['data_schema']['summary_col']\n",
    "\n",
    "clean_train_csv = os.path.join(DATA_DIR, 'train_clean.csv')\n",
    "clean_test_csv = os.path.join(DATA_DIR, 'test_clean.csv')\n",
    "train_df = pd.read_csv(clean_train_csv)\n",
    "test_df = pd.read_csv(clean_test_csv)\n",
    "\n",
    "ENABLE_CLEANING = True\n",
    "\n",
    "_ws_re = re.compile(r'\\s+')\n",
    "_docket_re = re.compile(r'\\b(\\d{1,4}[-/]\\d{1,6})(?:\\s*[A-Za-z]*)?\\b')\n",
    "\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    s = s.replace('\\u201c', '\"').replace('\\u201d', '\"').replace('\\u2019', \"'\")\n",
    "    s = _docket_re.sub(' ', s)\n",
    "    s = _ws_re.sub(' ', s)\n",
    "    s = s.strip()\n",
    "    if s and s[-1].isalnum():\n",
    "        s += '.'\n",
    "    return s\n",
    "\n",
    "if ENABLE_CLEANING:\n",
    "    train_df[in_col] = train_df[in_col].map(normalize_text)\n",
    "    if sum_col in train_df.columns:\n",
    "        train_df[sum_col] = train_df[sum_col].map(normalize_text)\n",
    "    if in_col in test_df.columns:\n",
    "        test_df[in_col] = test_df[in_col].map(normalize_text)\n",
    "    if sum_col in test_df.columns:\n",
    "        test_df[sum_col] = test_df[sum_col].map(normalize_text)\n",
    "\n",
    "# Overwrite cleaned files\n",
    "train_df.to_csv(clean_train_csv, index=False)\n",
    "test_df.to_csv(clean_test_csv, index=False)\n",
    "print('Re-saved normalized CSVs:', clean_train_csv, clean_test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f7e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Train/Validation Split and Persisted Splits\n",
    "import os, json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open(os.path.join(ROOT_DIR, 'pipeline_config.json'), 'r') as f:\n",
    "    CFG = json.load(f)\n",
    "\n",
    "in_col = CFG['data_schema']['input_col']\n",
    "sum_col = CFG['data_schema']['summary_col']\n",
    "\n",
    "clean_train_csv = os.path.join(DATA_DIR, 'train_clean.csv')\n",
    "train_df = pd.read_csv(clean_train_csv)\n",
    "\n",
    "train_split_csv = os.path.join(DATA_DIR, 'train_split.csv')\n",
    "val_split_csv = os.path.join(DATA_DIR, 'val_split.csv')\n",
    "\n",
    "if os.path.isfile(train_split_csv) and os.path.isfile(val_split_csv):\n",
    "    print('Using existing persisted splits:')\n",
    "    print(train_split_csv, val_split_csv)\n",
    "else:\n",
    "    # Create stratification bins by summary length if available\n",
    "    if sum_col in train_df.columns:\n",
    "        lengths = train_df[sum_col].fillna('').astype(str).str.split().map(len)\n",
    "        bins = pd.qcut(lengths, q=min(10, max(2, lengths.nunique())), duplicates='drop')\n",
    "        stratify = bins\n",
    "    else:\n",
    "        stratify = None\n",
    "    tr_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42, stratify=stratify)\n",
    "    tr_df.to_csv(train_split_csv, index=False)\n",
    "    val_df.to_csv(val_split_csv, index=False)\n",
    "\n",
    "print(pd.read_csv(train_split_csv).shape, pd.read_csv(val_split_csv).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7796eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Tokenizer Setup (pretrained + optional custom tokens)\n",
    "import os, json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "with open(os.path.join(ROOT_DIR, 'pipeline_config.json'), 'r') as f:\n",
    "    CFG = json.load(f)\n",
    "\n",
    "MODEL_CHECKPOINT = CFG['model']['checkpoint']\n",
    "TOKENIZER_DIR = CFG['io']['tokenizer_dir']\n",
    "\n",
    "# Load tokenizer\n",
    "print('Loading tokenizer:', MODEL_CHECKPOINT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, use_fast=True)\n",
    "\n",
    "# Optional: load argument tokens if available under SRC_DIR\n",
    "extra_tokens = []\n",
    "for fname in ['binary_tokens.txt', 'fine_grained_tokens.txt']:\n",
    "    p = os.path.join(SRC_DIR, 'summarization', fname)\n",
    "    if os.path.isfile(p):\n",
    "        with open(p, 'r') as f:\n",
    "            toks = [t.strip() for t in f if t.strip()]\n",
    "            extra_tokens.extend(toks)\n",
    "\n",
    "if extra_tokens:\n",
    "    print(f'Adding {len(extra_tokens)} special tokens from repo to tokenizer')\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': extra_tokens})\n",
    "\n",
    "# Persist tokenizer for reuse after restarts\n",
    "os.makedirs(TOKENIZER_DIR, exist_ok=True)\n",
    "tokenizer.save_pretrained(TOKENIZER_DIR)\n",
    "print('Saved tokenizer to', TOKENIZER_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c3dd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Dataset and Tokenization Functions (cache to disk)\n",
    "import os, json\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "with open(os.path.join(ROOT_DIR, 'pipeline_config.json'), 'r') as f:\n",
    "    CFG = json.load(f)\n",
    "\n",
    "in_col = CFG['data_schema']['input_col']\n",
    "sum_col = CFG['data_schema']['summary_col']\n",
    "max_src_len = CFG['data']['max_input_length']\n",
    "max_tgt_len = CFG['data']['max_output_length']\n",
    "\n",
    "train_split_csv = os.path.join(DATA_DIR, 'train_split.csv')\n",
    "val_split_csv = os.path.join(DATA_DIR, 'val_split.csv')\n",
    "clean_test_csv = os.path.join(DATA_DIR, 'test_clean.csv')\n",
    "\n",
    "# Load splits into HF Datasets\n",
    "train_ds = load_dataset('csv', data_files={'train': train_split_csv})['train']\n",
    "val_ds = load_dataset('csv', data_files={'train': val_split_csv})['train']\n",
    "test_ds = load_dataset('csv', data_files={'train': clean_test_csv})['train']\n",
    "\n",
    "print('Train/Val/Test sizes:', len(train_ds), len(val_ds), len(test_ds))\n",
    "\n",
    "# Tokenization function including global_attention_mask for LED\n",
    "from functools import partial\n",
    "\n",
    "def tokenize_fn(batch, tokenizer=None, in_col=None, sum_col=None, max_src_len=1024, max_tgt_len=256):\n",
    "    inputs = tokenizer(\n",
    "        batch[in_col],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_src_len,\n",
    "    )\n",
    "    model_inputs = {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "    }\n",
    "    if sum_col in batch:\n",
    "        labels = tokenizer(\n",
    "            batch[sum_col],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_tgt_len,\n",
    "        )['input_ids']\n",
    "        # Replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "        labels = [[(-100 if t == tokenizer.pad_token_id else t) for t in seq] for seq in labels]\n",
    "        model_inputs['labels'] = labels\n",
    "    # LED specific: global attention on first token\n",
    "    global_attention_mask = []\n",
    "    for att in inputs['attention_mask']:\n",
    "        gam = [0] * len(att)\n",
    "        if len(gam) > 0:\n",
    "            gam[0] = 1\n",
    "        global_attention_mask.append(gam)\n",
    "    model_inputs['global_attention_mask'] = global_attention_mask\n",
    "    return model_inputs\n",
    "\n",
    "map_fn = partial(\n",
    "    tokenize_fn,\n",
    "    tokenizer=tokenizer,\n",
    "    in_col=in_col,\n",
    "    sum_col=sum_col,\n",
    "    max_src_len=max_src_len,\n",
    "    max_tgt_len=max_tgt_len,\n",
    ")\n",
    "\n",
    "train_tokenized = train_ds.map(map_fn, batched=True, remove_columns=[c for c in train_ds.column_names if c not in [in_col, sum_col]])\n",
    "val_tokenized = val_ds.map(map_fn, batched=True, remove_columns=[c for c in val_ds.column_names if c not in [in_col, sum_col]])\n",
    "# For test, sum_col may be missing; keep only input processing\n",
    "map_fn_test = partial(tokenize_fn, tokenizer=tokenizer, in_col=in_col, sum_col=sum_col, max_src_len=max_src_len, max_tgt_len=max_tgt_len)\n",
    "test_tokenized = test_ds.map(map_fn_test, batched=True, remove_columns=[c for c in test_ds.column_names if c not in [in_col, sum_col] and c != in_col])\n",
    "\n",
    "print(train_tokenized[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e677bf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Data collator and dataset formatting\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=None, label_pad_token_id=-100, padding=True, return_tensors='pt')\n",
    "\n",
    "# set format for PyTorch\n",
    "train_tokenized.set_format(type='torch', columns=['input_ids','attention_mask','global_attention_mask','labels'])\n",
    "val_tokenized.set_format(type='torch', columns=['input_ids','attention_mask','global_attention_mask','labels'])\n",
    "# On test we might not have 'labels'\n",
    "cols_test = ['input_ids','attention_mask','global_attention_mask']\n",
    "cols_test = [c for c in cols_test if c in test_tokenized.column_names]\n",
    "test_tokenized.set_format(type='torch', columns=cols_test)\n",
    "\n",
    "print('Batch example shapes:')\n",
    "batch = data_collator([train_tokenized[i] for i in range(min(2, len(train_tokenized)))])\n",
    "for k, v in batch.items():\n",
    "    try:\n",
    "        print(k, v.shape)\n",
    "    except Exception:\n",
    "        print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbb6e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Initialize Encoder-Decoder Model (with LED defaults)\n",
    "import os, json, torch\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "with open(os.path.join(ROOT_DIR, 'pipeline_config.json'), 'r') as f:\n",
    "    CFG = json.load(f)\n",
    "\n",
    "MODEL_CHECKPOINT = CFG['model']['checkpoint']\n",
    "\n",
    "print('Loading model:', MODEL_CHECKPOINT)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "# If we added extra tokens to tokenizer, resize embeddings\n",
    "if hasattr(tokenizer, 'vocab'):\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# LED best practices for memory\n",
    "try:\n",
    "    model.gradient_checkpointing_enable()\n",
    "except Exception:\n",
    "    pass\n",
    "model.config.use_cache = False  # important with gradient checkpointing\n",
    "\n",
    "# Ensure EOS/BOS tokens are set (helps beam search stop cleanly)\n",
    "try:\n",
    "    if getattr(model.config, 'eos_token_id', None) is None and getattr(tokenizer, 'eos_token_id', None) is not None:\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "    if getattr(model.config, 'bos_token_id', None) is None and getattr(tokenizer, 'bos_token_id', None) is not None:\n",
    "        model.config.bos_token_id = tokenizer.bos_token_id\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Set default generation hyperparameters via generation_config (avoids warnings)\n",
    "# Align max_length to config to prevent premature truncation\n",
    "_gen_max = int(CFG['data'].get('max_output_length', 256))\n",
    "model.generation_config.num_beams = 4\n",
    "model.generation_config.max_length = _gen_max\n",
    "# Choose a reasonable min_length relative to the max (and ensure < max)\n",
    "model.generation_config.min_length = min(max(32, _gen_max // 6), _gen_max - 1)\n",
    "# Lower length penalty encourages finishing thoughts instead of early cut-offs\n",
    "model.generation_config.length_penalty = 1.0\n",
    "model.generation_config.early_stopping = True\n",
    "model.generation_config.no_repeat_ngram_size = 3\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    print('Moved model to CUDA')\n",
    "\n",
    "# Parameter counts\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Params: total={num_params:,} trainable={trainable_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bb65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) TrainingArguments with robust checkpointing/auto-resume\n",
    "import os, json\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "with open(os.path.join(ROOT_DIR, 'pipeline_config.json'), 'r') as f:\n",
    "    CFG = json.load(f)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=CKPT_DIR,\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    logging_steps=CFG['training'].get('logging_steps', 50),\n",
    "    eval_steps=CFG['training']['eval_steps'],\n",
    "    save_steps=CFG['training']['save_steps'],\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=CFG['training']['epochs'],\n",
    "    per_device_train_batch_size=CFG['model']['batchsize'],\n",
    "    per_device_eval_batch_size=CFG['model']['batchsize'],\n",
    "    gradient_accumulation_steps=CFG['training']['gradient_accumulation_steps'],\n",
    "    learning_rate=CFG['training']['learning_rate'],\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,\n",
    "    # Use a metric that exists in compute_metrics/evaluate output\n",
    "    metric_for_best_model='eval_rougeLsum',\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=['none'],  # disable wandb\n",
    "    # Generation defaults to keep Trainer.generate consistent with our model config\n",
    "    generation_max_length=CFG['data'].get('max_output_length', 256),\n",
    "    generation_num_beams=4,\n",
    "    # Stabilization\n",
    "    warmup_steps=CFG['training'].get('warmup_steps', 500),\n",
    "    label_smoothing_factor=CFG['training'].get('label_smoothing_factor', 0.1),\n",
    ")\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacdea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13) Start or Resume Training from Latest Checkpoint (restart-safe)\n",
    "import os, json\n",
    "from transformers import Seq2SeqTrainer, EarlyStoppingCallback\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "import evaluate, torch\n",
    "\n",
    "# Rehydrate state after crash\n",
    "ensure_state_for_training()\n",
    "\n",
    "cfg = load_cfg()\n",
    "rouge_metric = evaluate.load('rouge')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    gen_lens = [len(pred.split()) for pred in decoded_preds]\n",
    "    result['gen_len'] = sum(gen_lens) / max(1, len(gen_lens))\n",
    "    return result\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,  # use processing_class (replaces deprecated `tokenizer` arg)\n",
    "    args=args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "last_ckpt = None\n",
    "if os.path.isdir(CKPT_DIR):\n",
    "    last_ckpt = get_last_checkpoint(CKPT_DIR)\n",
    "    print('Last checkpoint:', last_ckpt)\n",
    "\n",
    "train_result = trainer.train(resume_from_checkpoint=last_ckpt)\n",
    "trainer.save_state()\n",
    "\n",
    "# Save training logs\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "train_logs_path = os.path.join(LOG_DIR, 'train_logs.txt')\n",
    "with open(train_logs_path, 'a') as f:\n",
    "    f.write(str(train_result.metrics) + '\\n')\n",
    "print('Training complete. Logs ->', train_logs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbc64b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) Evaluate with ROUGE on Validation and Test (restart-safe)\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Rehydrate state\n",
    "ensure_state_for_training()\n",
    "CFG = load_cfg_ensured()\n",
    "in_col = CFG['data_schema']['input_col']\n",
    "sum_col = CFG['data_schema']['summary_col']\n",
    "\n",
    "# Validation evaluation (use default 'eval_' prefix to avoid key mismatches)\n",
    "val_metrics = trainer.evaluate(eval_dataset=val_tokenized)\n",
    "print('Validation metrics:', val_metrics)\n",
    "\n",
    "# Test evaluation\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, 'test_clean.csv'))\n",
    "has_test_refs = (sum_col in test_df.columns) and test_df[sum_col].notna().any() and ('labels' in getattr(test_tokenized, 'column_names', []))\n",
    "\n",
    "if has_test_refs:\n",
    "    # We have references -> compute ROUGE with 'test_' prefix\n",
    "    test_metrics = trainer.evaluate(eval_dataset=test_tokenized, metric_key_prefix='test')\n",
    "else:\n",
    "    # No references -> disable compute_metrics to avoid KeyErrors and just run predict for timing\n",
    "    saved_cm = trainer.compute_metrics\n",
    "    trainer.compute_metrics = None\n",
    "    pred = trainer.predict(test_tokenized, metric_key_prefix='test')\n",
    "    trainer.compute_metrics = saved_cm\n",
    "    test_metrics = pred.metrics\n",
    "\n",
    "print('Test metrics:', test_metrics)\n",
    "\n",
    "# Helper to safely cast numbers for JSON\n",
    "def _to_float_dict(d):\n",
    "    out = {}\n",
    "    for k, v in d.items():\n",
    "        try:\n",
    "            out[k] = float(v)\n",
    "        except Exception:\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "# Save metrics\n",
    "with open(os.path.join(ROOT_DIR, 'metrics_val.json'), 'w') as f:\n",
    "    json.dump(_to_float_dict(val_metrics), f, indent=2)\n",
    "with open(os.path.join(ROOT_DIR, 'metrics_test.json'), 'w') as f:\n",
    "    json.dump(_to_float_dict(test_metrics), f, indent=2)\n",
    "print('Saved metrics to ROOT_DIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5454bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) Save Best Model/Tokenizer to Drive and verify reload\n",
    "import os\n",
    "\n",
    "trainer.save_model(BEST_DIR)  # saves model + tokenizer (if passed) + training args\n",
    "try:\n",
    "    tokenizer.save_pretrained(BEST_DIR)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print('Saved best model to', BEST_DIR)\n",
    "\n",
    "# Quick reload test\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "re_tok = AutoTokenizer.from_pretrained(BEST_DIR)\n",
    "re_model = AutoModelForSeq2SeqLM.from_pretrained(BEST_DIR)\n",
    "if torch.cuda.is_available():\n",
    "    re_model = re_model.cuda()\n",
    "\n",
    "dummy = tokenizer('Test input.', return_tensors='pt')\n",
    "if torch.cuda.is_available():\n",
    "    dummy = {k: v.cuda() for k, v in dummy.items()}\n",
    "out = re_model.generate(**dummy, max_length=32, num_beams=2)\n",
    "print('Reloaded model generated:', re_tok.batch_decode(out, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa8d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16) Batch Inference on Test Set and Export Summaries (restart-safe)\n",
    "import os, json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "CFG = load_cfg_ensured()\n",
    "\n",
    "# Reload best tokenizer/model for inference\n",
    "infer_tokenizer = load_tokenizer_for_inference(CFG)\n",
    "infer_model = load_model_for_inference(CFG, tokenizer=infer_tokenizer)\n",
    "\n",
    "in_col_cfg = CFG['data_schema']['input_col']\n",
    "sum_col = CFG['data_schema']['summary_col']  # may be absent in test\n",
    "\n",
    "# Optional quick override for longer outputs (no retrain needed)\n",
    "# You can also set GEN_MAX_LEN env var to control this dynamically.\n",
    "OVERRIDE_MAX_LEN = int(os.environ.get('GEN_MAX_LEN', '640'))  # try 640 or 768\n",
    "if OVERRIDE_MAX_LEN > 0:\n",
    "    infer_model.generation_config.max_length = OVERRIDE_MAX_LEN\n",
    "    # keep a reasonable min_length but ensure < max\n",
    "    infer_model.generation_config.min_length = min(max(32, OVERRIDE_MAX_LEN // 6), OVERRIDE_MAX_LEN - 1)\n",
    "    infer_model.generation_config.length_penalty = 1.0\n",
    "\n",
    "print(\n",
    "    'Generation config ->',\n",
    "    'max_length=', infer_model.generation_config.max_length,\n",
    "    '| min_length=', infer_model.generation_config.min_length,\n",
    "    '| num_beams=', infer_model.generation_config.num_beams,\n",
    "    '| length_penalty=', infer_model.generation_config.length_penalty,\n",
    ")\n",
    "\n",
    "# Reload test CSV for text and reference\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, 'test_clean.csv'))\n",
    "# Ensure lowercase columns (safety)\n",
    "test_df.columns = [str(c).strip().lower() for c in test_df.columns]\n",
    "\n",
    "# Determine effective input column for test\n",
    "candidates = [in_col_cfg] + CFG['data_schema'].get('input_col_candidates', []) + ['text', 'article', 'document', 'content']\n",
    "effective_in_col = next((c for c in candidates if c in test_df.columns), None)\n",
    "if effective_in_col is None:\n",
    "    raise KeyError(f\"None of the candidate input columns {list(dict.fromkeys(candidates))} were found in test_clean.csv. Available columns: {list(test_df.columns)}\")\n",
    "\n",
    "# Generation parameters (align with generation_config)\n",
    "gen_kwargs = dict(\n",
    "    max_length=infer_model.generation_config.max_length,\n",
    "    min_length=infer_model.generation_config.min_length,\n",
    "    num_beams=infer_model.generation_config.num_beams,\n",
    "    length_penalty=infer_model.generation_config.length_penalty,\n",
    "    no_repeat_ngram_size=infer_model.generation_config.no_repeat_ngram_size,\n",
    ")\n",
    "\n",
    "preds = []\n",
    "bsz = 4\n",
    "for i in tqdm(range(0, len(test_df), bsz)):\n",
    "    batch_texts = test_df[effective_in_col].iloc[i:i+bsz].tolist()\n",
    "    inputs = infer_tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=CFG['data']['max_input_length'])\n",
    "    # LED: add global attention on first token\n",
    "    attn = inputs['attention_mask']\n",
    "    global_attention_mask = torch.zeros_like(attn)\n",
    "    global_attention_mask[:, 0] = 1\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        global_attention_mask = global_attention_mask.cuda()\n",
    "    with torch.no_grad():\n",
    "        out_ids = infer_model.generate(**inputs, global_attention_mask=global_attention_mask, **gen_kwargs)\n",
    "    gen_texts = infer_tokenizer.batch_decode(out_ids, skip_special_tokens=True)\n",
    "    preds.extend(gen_texts)\n",
    "\n",
    "export_path = os.path.join(EXPORT_DIR, 'test_predictions.csv')\n",
    "export_df = pd.DataFrame({\n",
    "    'id': range(len(preds)),\n",
    "    'text': test_df[effective_in_col].tolist(),\n",
    "    'generated_summary': preds,\n",
    "})\n",
    "if sum_col in test_df.columns:\n",
    "    export_df['reference_summary'] = test_df[sum_col].tolist()\n",
    "\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "export_df.to_csv(export_path, index=False)\n",
    "print('Saved predictions to', export_path)\n",
    "\n",
    "# Quick peek\n",
    "export_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea30eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17) Ad-hoc Inference on Custom Legal Texts (restart-safe)\n",
    "CFG = load_cfg_ensured()\n",
    "\n",
    "# Reload best tokenizer/model for inference\n",
    "infer_tokenizer = load_tokenizer_for_inference(CFG)\n",
    "infer_model = load_model_for_inference(CFG, tokenizer=infer_tokenizer)\n",
    "\n",
    "# Optional quick override for longer outputs (no retrain needed)\n",
    "OVERRIDE_MAX_LEN = int(os.environ.get('GEN_MAX_LEN', '640'))  # try 640 or 768\n",
    "if OVERRIDE_MAX_LEN > 0:\n",
    "    infer_model.generation_config.max_length = OVERRIDE_MAX_LEN\n",
    "    infer_model.generation_config.min_length = min(max(32, OVERRIDE_MAX_LEN // 6), OVERRIDE_MAX_LEN - 1)\n",
    "    infer_model.generation_config.length_penalty = 1.0\n",
    "\n",
    "print(\n",
    "    'Generation config ->',\n",
    "    'max_length=', infer_model.generation_config.max_length,\n",
    "    '| min_length=', infer_model.generation_config.min_length,\n",
    "    '| num_beams=', infer_model.generation_config.num_beams,\n",
    "    '| length_penalty=', infer_model.generation_config.length_penalty,\n",
    ")\n",
    "\n",
    "samples = [\n",
    "    \"The appellant challenges the lower court's ruling on evidentiary grounds, alleging improper admission of hearsay statements.\",\n",
    "    \"In this contractual dispute, the plaintiff claims breach due to non-delivery within the stipulated time frame, seeking damages and specific performance.\",\n",
    "]\n",
    "\n",
    "inputs = infer_tokenizer(samples, return_tensors='pt', padding=True, truncation=True, max_length=CFG['data']['max_input_length'])\n",
    "attn = inputs['attention_mask']\n",
    "global_attention_mask = torch.zeros_like(attn)\n",
    "global_attention_mask[:, 0] = 1\n",
    "if torch.cuda.is_available():\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    global_attention_mask = global_attention_mask.cuda()\n",
    "\n",
    "out_ids = infer_model.generate(\n",
    "    **inputs,\n",
    "    global_attention_mask=global_attention_mask,\n",
    "    max_length=infer_model.generation_config.max_length,\n",
    "    min_length=infer_model.generation_config.min_length,\n",
    "    num_beams=infer_model.generation_config.num_beams,\n",
    "    length_penalty=infer_model.generation_config.length_penalty,\n",
    ")\n",
    "for src, pred in zip(samples, infer_tokenizer.batch_decode(out_ids, skip_special_tokens=True)):\n",
    "    print('\\nSOURCE:\\n', src)\n",
    "    print('\\nSUMMARY:\\n', pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4525b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) Optional: Compute ROUGE via src/summarization/score_summaries.py (cross-validate)\n",
    "import os, subprocess, sys\n",
    "\n",
    "summ_script = os.path.join(SRC_DIR, 'summarization', 'score_summaries.py')\n",
    "export_path = os.path.join(EXPORT_DIR, 'test_predictions.csv')\n",
    "\n",
    "if os.path.isfile(summ_script) and os.path.isfile(export_path):\n",
    "    print('Running SummEval-based scoring...')\n",
    "    try:\n",
    "        # summ_eval requires additional install; install quietly if missing\n",
    "        import summ_eval  # type: ignore\n",
    "    except Exception:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'SummEval'])\n",
    "    # Run as a module-style call using Python\n",
    "    cmd = [sys.executable, summ_script, '-summary_out', export_path]\n",
    "    print('Command:', ' '.join(cmd))\n",
    "    rc = subprocess.call(cmd)\n",
    "    if rc != 0:\n",
    "        print('SummEval scoring script returned non-zero exit code:', rc)\n",
    "else:\n",
    "    print('Skipping SummEval scoring (script or export not found).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d703e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19) Optional: Argument signals from src/argument_classification (augment inputs)\n",
    "import os, pandas as pd\n",
    "\n",
    "preds_path = os.path.join(SRC_DIR, 'argument_classification', 'artifacts', 'legal_bert_predicts.txt')\n",
    "if os.path.isfile(preds_path):\n",
    "    print('Found argument predictions at', preds_path)\n",
    "    # Example augmentation: prepend a marker indicating argumentative sections\n",
    "    # This is a placeholder illustrating how to integrate such features\n",
    "    # You could merge these predictions onto your train/test by ID and modify text inputs accordingly.\n",
    "else:\n",
    "    print('No argument classification artifacts found; skipping augmentation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b17fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Set seeds for reproducibility\n",
    "import random, os\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import set_seed\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "set_seed(SEED)\n",
    "print('Seeds set to', SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcf610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Bootstrap functions to rehydrate state after a crash\n",
    "import os, json, torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_cfg():\n",
    "    with open(os.path.join(ROOT_DIR, 'pipeline_config.json'), 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def _resolve_cols_from_df(df, cfg):\n",
    "    cols_lower = [str(c).strip().lower() for c in df.columns]\n",
    "    # Prefer configured candidates\n",
    "    in_c = None\n",
    "    sum_c = None\n",
    "    for c in cfg['data_schema'].get('input_col_candidates', []):\n",
    "        if c.lower() in cols_lower:\n",
    "            in_c = c.lower()\n",
    "            break\n",
    "    for c in cfg['data_schema'].get('summary_col_candidates', []):\n",
    "        if c.lower() in cols_lower:\n",
    "            sum_c = c.lower()\n",
    "            break\n",
    "    # Common fallbacks\n",
    "    if in_c is None and 'text' in cols_lower:\n",
    "        in_c = 'text'\n",
    "    if sum_c is None and 'summary' in cols_lower:\n",
    "        sum_c = 'summary'\n",
    "    # Last resort fallback: pick the first textual-looking column\n",
    "    if in_c is None and len(cols_lower) > 0:\n",
    "        in_c = cols_lower[0]\n",
    "    return in_c, sum_c\n",
    "\n",
    "\n",
    "def ensure_cfg_schema(cfg):\n",
    "    ds = cfg.get('data_schema', {})\n",
    "    needs = ('input_col' not in ds) or ('summary_col' not in ds)\n",
    "    if not needs:\n",
    "        return cfg\n",
    "    # Try to infer from cleaned train csv, then raw train, then any csv in DATA_DIR\n",
    "    candidate_paths = [\n",
    "        os.path.join(DATA_DIR, 'train_clean.csv'),\n",
    "        os.path.join(DATA_DIR, 'train_split.csv'),\n",
    "        os.path.join(DATA_DIR, 'train.csv'),\n",
    "    ]\n",
    "    # Add any other CSVs in DATA_DIR as last resort\n",
    "    for fname in os.listdir(DATA_DIR):\n",
    "        if fname.lower().endswith('.csv'):\n",
    "            p = os.path.join(DATA_DIR, fname)\n",
    "            if p not in candidate_paths:\n",
    "                candidate_paths.append(p)\n",
    "    for p in candidate_paths:\n",
    "        try:\n",
    "            if os.path.isfile(p):\n",
    "                df = pd.read_csv(p, nrows=100)\n",
    "                df.columns = [str(c).strip().lower() for c in df.columns]\n",
    "                in_c, sum_c = _resolve_cols_from_df(df, cfg)\n",
    "                if in_c is not None:\n",
    "                    cfg['data_schema']['input_col'] = in_c\n",
    "                if sum_c is not None:\n",
    "                    cfg['data_schema']['summary_col'] = sum_c\n",
    "                # Persist and return if input_col found\n",
    "                if in_c is not None:\n",
    "                    with open(os.path.join(ROOT_DIR, 'pipeline_config.json'), 'w') as f:\n",
    "                        json.dump(cfg, f, indent=2)\n",
    "                    print(f\"Schema ensured from {os.path.basename(p)} -> input_col='{in_c}', summary_col='{sum_c}'\")\n",
    "                    return cfg\n",
    "        except Exception:\n",
    "            pass\n",
    "    # If still not found, leave as-is (downstream will raise a clearer error)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def load_cfg_ensured():\n",
    "    cfg = load_cfg()\n",
    "    cfg = ensure_cfg_schema(cfg)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def load_tokenizer_for_inference(cfg):\n",
    "    # Prefer best dir -> tokenizer dir -> checkpoint\n",
    "    if os.path.isdir(BEST_DIR):\n",
    "        try:\n",
    "            return AutoTokenizer.from_pretrained(BEST_DIR, use_fast=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if os.path.isdir(cfg['io']['tokenizer_dir']):\n",
    "        try:\n",
    "            return AutoTokenizer.from_pretrained(cfg['io']['tokenizer_dir'], use_fast=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return AutoTokenizer.from_pretrained(cfg['model']['checkpoint'], use_fast=True)\n",
    "\n",
    "\n",
    "def _apply_generation_defaults(model):\n",
    "    # Keep generation behavior consistent across reloads (use CFG if available)\n",
    "    try:\n",
    "        with open(os.path.join(ROOT_DIR, 'pipeline_config.json'), 'r') as _f:\n",
    "            _cfg = json.load(_f)\n",
    "        _gen_max = int(_cfg.get('data', {}).get('max_output_length', 256))\n",
    "    except Exception:\n",
    "        _gen_max = 256\n",
    "    model.generation_config.num_beams = 4\n",
    "    model.generation_config.max_length = _gen_max\n",
    "    model.generation_config.min_length = min(max(32, _gen_max // 6), _gen_max - 1)\n",
    "    model.generation_config.length_penalty = 1.0\n",
    "    model.generation_config.early_stopping = True\n",
    "    model.generation_config.no_repeat_ngram_size = 3\n",
    "\n",
    "\n",
    "def load_model_for_inference(cfg, tokenizer=None):\n",
    "    path = BEST_DIR if os.path.isdir(BEST_DIR) else cfg['model']['checkpoint']\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(path)\n",
    "    if tokenizer is not None:\n",
    "        try:\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    except Exception:\n",
    "        pass\n",
    "    model.config.use_cache = False\n",
    "    _apply_generation_defaults(model)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    return model\n",
    "\n",
    "\n",
    "def prepare_tokenized_datasets(cfg, tokenizer):\n",
    "    in_col = cfg['data_schema']['input_col']\n",
    "    sum_col = cfg['data_schema']['summary_col']\n",
    "    max_src_len = cfg['data']['max_input_length']\n",
    "    max_tgt_len = cfg['data']['max_output_length']\n",
    "\n",
    "    train_split_csv = os.path.join(DATA_DIR, 'train_split.csv')\n",
    "    val_split_csv = os.path.join(DATA_DIR, 'val_split.csv')\n",
    "    clean_test_csv = os.path.join(DATA_DIR, 'test_clean.csv')\n",
    "\n",
    "    train_ds = load_dataset('csv', data_files={'train': train_split_csv})['train']\n",
    "    val_ds = load_dataset('csv', data_files={'train': val_split_csv})['train']\n",
    "    test_ds = load_dataset('csv', data_files={'train': clean_test_csv})['train']\n",
    "\n",
    "    def tokenize_fn(batch, tokenizer=None, in_col=None, sum_col=None, max_src_len=1024, max_tgt_len=256):\n",
    "        inputs = tokenizer(\n",
    "            batch[in_col],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_src_len,\n",
    "        )\n",
    "        model_inputs = {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask'],\n",
    "        }\n",
    "        if sum_col in batch:\n",
    "            labels = tokenizer(\n",
    "                batch[sum_col],\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=max_tgt_len,\n",
    "            )['input_ids']\n",
    "            labels = [[(-100 if t == tokenizer.pad_token_id else t) for t in seq] for seq in labels]\n",
    "            model_inputs['labels'] = labels\n",
    "        # LED: global attention on first token\n",
    "        global_attention_mask = []\n",
    "        for att in inputs['attention_mask']:\n",
    "            gam = [0] * len(att)\n",
    "            if len(gam) > 0:\n",
    "                gam[0] = 1\n",
    "            global_attention_mask.append(gam)\n",
    "        model_inputs['global_attention_mask'] = global_attention_mask\n",
    "        return model_inputs\n",
    "\n",
    "    map_fn = partial(\n",
    "        tokenize_fn,\n",
    "        tokenizer=tokenizer,\n",
    "        in_col=in_col,\n",
    "        sum_col=sum_col,\n",
    "        max_src_len=max_src_len,\n",
    "        max_tgt_len=max_tgt_len,\n",
    "    )\n",
    "    train_tok = train_ds.map(map_fn, batched=True, remove_columns=[c for c in train_ds.column_names if c not in [in_col, sum_col]])\n",
    "    val_tok = val_ds.map(map_fn, batched=True, remove_columns=[c for c in val_ds.column_names if c not in [in_col, sum_col]])\n",
    "    test_tok = test_ds.map(map_fn, batched=True, remove_columns=[c for c in test_ds.column_names if c not in [in_col, sum_col] and c != in_col])\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=None, label_pad_token_id=-100, padding=True, return_tensors='pt')\n",
    "\n",
    "    train_tok.set_format(type='torch', columns=['input_ids','attention_mask','global_attention_mask','labels'])\n",
    "    val_tok.set_format(type='torch', columns=['input_ids','attention_mask','global_attention_mask','labels'])\n",
    "    cols_test = ['input_ids','attention_mask','global_attention_mask']\n",
    "    cols_test = [c for c in cols_test if c in test_tok.column_names]\n",
    "    test_tok.set_format(type='torch', columns=cols_test)\n",
    "\n",
    "    return train_tok, val_tok, test_tok, data_collator\n",
    "\n",
    "\n",
    "def ensure_state_for_training():\n",
    "    # Ensures tokenizer, model, train_tokenized, val_tokenized, data_collator exist in globals\n",
    "    g = globals()\n",
    "    cfg = load_cfg_ensured()\n",
    "    if 'tokenizer' not in g or g['tokenizer'] is None:\n",
    "        g['tokenizer'] = load_tokenizer_for_inference(cfg)\n",
    "    if 'model' not in g or g['model'] is None:\n",
    "        g['model'] = AutoModelForSeq2SeqLM.from_pretrained(cfg['model']['checkpoint'])\n",
    "        try:\n",
    "            g['model'].resize_token_embeddings(len(g['tokenizer']))\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            g['model'].gradient_checkpointing_enable()\n",
    "        except Exception:\n",
    "            pass\n",
    "        g['model'].config.use_cache = False\n",
    "        _apply_generation_defaults(g['model'])\n",
    "        if torch.cuda.is_available():\n",
    "            g['model'] = g['model'].cuda()\n",
    "    needed = any(name not in g or g[name] is None for name in ['train_tokenized', 'val_tokenized', 'data_collator'])\n",
    "    if needed:\n",
    "        tr, va, te, dc = prepare_tokenized_datasets(cfg, g['tokenizer'])\n",
    "        g['train_tokenized'] = tr\n",
    "        g['val_tokenized'] = va\n",
    "        g['test_tokenized'] = te\n",
    "        g['data_collator'] = dc\n",
    "\n",
    "print('Bootstrap utilities ready. Use ensure_state_for_training() in cells to rehydrate.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25455599",
   "metadata": {},
   "source": [
    "# 20) Notes on warnings you may see\n",
    "- The FutureWarning about `tokenizer` in Trainer is now resolved by using `processing_class`.\n",
    "- A message about moving generation attributes to `generation_config` is expected; we now set them directly to avoid it.\n",
    "- If you see a warning about missing keys like `embed_tokens.weight` or `lm_head.weight` when resuming or reloading, it's typically harmless when we've resized token embeddings after adding custom tokens. Those extra rows are newly initialized if the checkpoint was created before the tokenizer grew. To avoid it, keep the tokenizer vocabulary stable across runs (we persist it under `TOKENIZER_DIR`)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
